import type { IpcMain } from "electron";
import type Database from "better-sqlite3";

import type { IpcResponse } from "../../../../../packages/shared/types/ipc-generated";
import {
  SKILL_STREAM_CHUNK_CHANNEL,
  SKILL_STREAM_DONE_CHANNEL,
  type AiStreamEvent,
} from "../../../../../packages/shared/types/ai";
import type { Logger } from "../logging/logger";
import { createIpcPushBackpressureGate } from "./pushBackpressure";
import { createAiService } from "../services/ai/aiService";
import {
  type SecretStorageAdapter,
  createAiProxySettingsService,
} from "../services/ai/aiProxySettingsService";
import { createMemoryService } from "../services/memory/memoryService";
import {
  recordSkillFeedbackAndLearn,
  type SkillFeedbackAction,
} from "../services/memory/preferenceLearning";
import { createStatsService } from "../services/stats/statsService";
import { createSkillService } from "../services/skills/skillService";
import { createSkillExecutor } from "../services/skills/skillExecutor";
import { createContextLayerAssemblyService } from "../services/context/layerAssemblyService";
import { createDbNotReadyError } from "./dbError";

type SkillRunPayload = {
  skillId: string;
  input: string;
  mode: "agent" | "plan" | "ask";
  model: string;
  candidateCount?: number;
  context?: { projectId?: string; documentId?: string };
  promptDiagnostics?: { stablePrefixHash: string; promptHash: string };
  stream: boolean;
};

type SkillRunUsage = {
  promptTokens: number;
  completionTokens: number;
  sessionTotalTokens: number;
  estimatedCostUsd?: number;
};

type SkillRunCandidate = {
  id: string;
  runId: string;
  text: string;
  summary: string;
};

type SkillRunResponse = {
  executionId: string;
  runId: string;
  outputText?: string;
  candidates?: SkillRunCandidate[];
  usage?: SkillRunUsage;
  promptDiagnostics?: { stablePrefixHash: string; promptHash: string };
};

type SkillRunResponseDataInput = SkillRunResponse & {
  contextPrompt?: string;
};

type ModelCatalogResponse = {
  source: "proxy" | "openai" | "anthropic";
  items: Array<{ id: string; name: string; provider: string }>;
};

type SkillFeedbackPayload = {
  runId: string;
  action: SkillFeedbackAction;
  evidenceRef: string;
};

type SkillFeedbackResponse = {
  recorded: true;
  learning?: {
    ignored: boolean;
    ignoredReason?: string;
    learned: boolean;
    learnedMemoryId?: string;
    signalCount?: number;
    threshold?: number;
  };
};

type ChatSendPayload = {
  message: string;
  projectId?: string;
  documentId?: string;
};

type ChatSendResponse = {
  accepted: true;
  messageId: string;
  echoed: string;
};

type ChatListPayload = {
  projectId?: string;
};

type ChatMessageRole = "user" | "assistant";

type ChatHistoryMessage = {
  messageId: string;
  projectId: string;
  role: ChatMessageRole;
  content: string;
  skillId?: string;
  timestamp: number;
  traceId: string;
};

type ChatListResponse = {
  items: ChatHistoryMessage[];
};

type ChatClearPayload = {
  projectId?: string;
};

type ChatClearResponse = {
  cleared: true;
  removed: number;
};

const AI_STREAM_RATE_LIMIT_PER_SECOND = 5_000;
const AI_CANDIDATE_COUNT_MIN = 1;
const AI_CANDIDATE_COUNT_MAX = 5;
const AI_CHAT_MESSAGE_CAPACITY = 2_000;

type ModelPricing = {
  promptPer1kTokens: number;
  completionPer1kTokens: number;
};

/**
 * Return an epoch-ms timestamp for AI stream events.
 */
function nowTs(): number {
  return Date.now();
}

/**
 * Estimate token usage deterministically from UTF-8 bytes.
 *
 * Why: keep usage accounting stable without introducing provider-specific tokenizers.
 */
function estimateTokenCount(text: string): number {
  if (text.length === 0) {
    return 0;
  }
  return Math.max(1, Math.ceil(Buffer.byteLength(text, "utf8") / 4));
}

/**
 * Parse candidateCount input and enforce the fixed 1..5 range.
 */
function parseCandidateCount(
  raw: number | undefined,
):
  | { ok: true; data: number }
  | { ok: false; error: { code: "INVALID_ARGUMENT"; message: string } } {
  if (raw === undefined) {
    return { ok: true, data: 1 };
  }
  if (!Number.isFinite(raw) || !Number.isInteger(raw)) {
    return {
      ok: false,
      error: {
        code: "INVALID_ARGUMENT",
        message: "candidateCount must be an integer between 1 and 5",
      },
    };
  }
  if (raw < AI_CANDIDATE_COUNT_MIN || raw > AI_CANDIDATE_COUNT_MAX) {
    return {
      ok: false,
      error: {
        code: "INVALID_ARGUMENT",
        message: "candidateCount must be between 1 and 5",
      },
    };
  }
  return { ok: true, data: raw };
}

/**
 * Build a concise card summary for candidate rendering.
 */
function summarizeCandidateText(text: string): string {
  const normalized = text.replace(/\s+/g, " ").trim();
  if (normalized.length <= 120) {
    return normalized;
  }
  return `${normalized.slice(0, 117)}...`;
}

/**
 * Normalize AI run response payload to the IPC contract surface.
 *
 * Why: executor internals (e.g. `contextPrompt`) must never leak across IPC
 * response validation boundaries.
 */
export function toSkillRunResponseData(
  data: SkillRunResponseDataInput,
): SkillRunResponse {
  return {
    executionId: data.executionId,
    runId: data.runId,
    ...(typeof data.outputText === "string"
      ? { outputText: data.outputText }
      : {}),
    ...(Array.isArray(data.candidates) ? { candidates: data.candidates } : {}),
    ...(data.usage ? { usage: data.usage } : {}),
    ...(data.promptDiagnostics
      ? { promptDiagnostics: data.promptDiagnostics }
      : {}),
  };
}

/**
 * Normalize skill id to the leaf token.
 */
function leafSkillId(skillId: string): string {
  const parts = skillId.split(":");
  return parts[parts.length - 1] ?? skillId;
}

/**
 * Derive deterministic prompt-token input text for usage accounting.
 */
function promptInputForUsage(payload: SkillRunPayload): string {
  if (payload.input.trim().length > 0) {
    return payload.input;
  }
  if (leafSkillId(payload.skillId) === "continue") {
    return "请基于当前文档上下文继续写作。";
  }
  return payload.input;
}

/**
 * Parse per-model pricing from env JSON.
 *
 * Format:
 * {"gpt-5.2":{"promptPer1kTokens":0.0015,"completionPer1kTokens":0.003}}
 */
function parseModelPricingMap(
  env: NodeJS.ProcessEnv,
): Map<string, ModelPricing> {
  const raw = env.CREONOW_AI_MODEL_PRICING_JSON;
  if (typeof raw !== "string" || raw.trim().length === 0) {
    return new Map();
  }
  try {
    const parsed = JSON.parse(raw) as unknown;
    if (!parsed || typeof parsed !== "object" || Array.isArray(parsed)) {
      return new Map();
    }
    const entries = Object.entries(parsed as Record<string, unknown>);
    const map = new Map<string, ModelPricing>();
    for (const [model, value] of entries) {
      if (!value || typeof value !== "object" || Array.isArray(value)) {
        continue;
      }
      const record = value as Record<string, unknown>;
      const prompt = record.promptPer1kTokens;
      const completion = record.completionPer1kTokens;
      if (
        typeof prompt !== "number" ||
        !Number.isFinite(prompt) ||
        prompt < 0 ||
        typeof completion !== "number" ||
        !Number.isFinite(completion) ||
        completion < 0
      ) {
        continue;
      }
      map.set(model, {
        promptPer1kTokens: prompt,
        completionPer1kTokens: completion,
      });
    }
    return map;
  } catch {
    return new Map();
  }
}

/**
 * Validate and normalize chat project scope key.
 *
 * Why: chat history must be isolated by project to prevent cross-project leakage.
 */
function resolveChatProjectId(args: {
  projectId?: string;
}):
  | { ok: true; data: string }
  | { ok: false; error: { code: "INVALID_ARGUMENT"; message: string } } {
  const projectId = args.projectId?.trim() ?? "";
  if (projectId.length === 0) {
    return {
      ok: false,
      error: {
        code: "INVALID_ARGUMENT",
        message: "projectId is required",
      },
    };
  }
  return { ok: true, data: projectId };
}

/**
 * Best-effort emit a stream event to the renderer that invoked the skill.
 *
 * Why: renderer cannot access Node APIs; streaming must cross IPC as push events.
 */
function safeEmitToRenderer(args: {
  logger: Logger;
  sender: Electron.WebContents;
  event: AiStreamEvent;
}): void {
  const channel =
    args.event.type === "chunk"
      ? SKILL_STREAM_CHUNK_CHANNEL
      : SKILL_STREAM_DONE_CHANNEL;
  try {
    args.sender.send(channel, args.event);
  } catch (error) {
    args.logger.error("ai_stream_send_failed", {
      message: error instanceof Error ? error.message : String(error),
    });
  }
}

/**
 * Register `ai:skill:*` IPC handlers.
 *
 * Why: AI runtime lives in the main process (secrets + network + observability),
 * while the renderer only consumes typed results and stream events.
 */
export function registerAiIpcHandlers(deps: {
  ipcMain: IpcMain;
  db: Database.Database | null;
  userDataDir: string;
  builtinSkillsDir: string;
  logger: Logger;
  env: NodeJS.ProcessEnv;
  secretStorage?: SecretStorageAdapter;
}): void {
  const pushBackpressureByRenderer = new Map<
    number,
    ReturnType<typeof createIpcPushBackpressureGate>
  >();

  /**
   * Resolve per-renderer push backpressure gate.
   *
   * Why: rate limit must be isolated by renderer process, avoiding cross-window coupling.
   */
  function getPushBackpressureGate(
    sender: Electron.WebContents,
  ): ReturnType<typeof createIpcPushBackpressureGate> {
    const existing = pushBackpressureByRenderer.get(sender.id);
    if (existing) {
      return existing;
    }

    const created = createIpcPushBackpressureGate({
      limitPerSecond: AI_STREAM_RATE_LIMIT_PER_SECOND,
      onDrop: (event) => {
        deps.logger.info("ipc_push_backpressure_triggered", {
          rendererId: sender.id,
          channel: SKILL_STREAM_CHUNK_CHANNEL,
          timestamp: event.timestamp,
          droppedInWindow: event.droppedInWindow,
          limitPerSecond: event.limitPerSecond,
        });
      },
    });
    pushBackpressureByRenderer.set(sender.id, created);
    return created;
  }

  const aiService = createAiService({
    logger: deps.logger,
    env: deps.env,
    getProxySettings: () => {
      if (!deps.db) {
        return null;
      }
      const svc = createAiProxySettingsService({
        db: deps.db,
        logger: deps.logger,
        secretStorage: deps.secretStorage,
      });
      const res = svc.getRaw();
      return res.ok ? res.data : null;
    },
  });
  const runRegistry = new Map<
    string,
    { startedAt: number; context?: SkillRunPayload["context"] }
  >();
  const chatHistoryByProject = new Map<string, ChatHistoryMessage[]>();
  const sessionTokenTotalsByContext = new Map<string, number>();
  const modelPricingByModel = parseModelPricingMap(deps.env);
  const contextAssemblyService = createContextLayerAssemblyService(undefined);
  const skillExecutor = createSkillExecutor({
    resolveSkill: (skillId) => {
      if (!deps.db) {
        return {
          ok: false,
          error: createDbNotReadyError(),
        };
      }
      const skillSvc = createSkillService({
        db: deps.db,
        userDataDir: deps.userDataDir,
        builtinSkillsDir: deps.builtinSkillsDir,
        logger: deps.logger,
      });
      const resolved = skillSvc.resolveForRun({ id: skillId });
      if (!resolved.ok) {
        return {
          ok: false,
          error: resolved.error,
        };
      }
      return {
        ok: true,
        data: {
          id: resolved.data.skill.id,
          prompt: resolved.data.skill.prompt,
          enabled: resolved.data.enabled,
          valid: resolved.data.skill.valid,
          error_code: resolved.data.skill.error_code,
          error_message: resolved.data.skill.error_message,
        },
      };
    },
    runSkill: async (args) => {
      return await aiService.runSkill(args);
    },
    assembleContext: async (args) => {
      return await contextAssemblyService.assemble(args);
    },
  });

  /**
   * Remember a runId for feedback validation.
   *
   * Why: feedback can arrive after the underlying in-flight run entry is cleaned up.
   */
  function rememberRunId(args: {
    runId: string;
    context?: SkillRunPayload["context"];
  }): void {
    runRegistry.set(args.runId, { startedAt: nowTs(), context: args.context });

    const cutoff = nowTs() - 24 * 60 * 60 * 1000;
    for (const [runId, entry] of runRegistry) {
      if (entry.startedAt < cutoff) {
        runRegistry.delete(runId);
      }
    }
  }

  /**
   * Resolve a deterministic usage aggregation key.
   *
   * Why: session token totals must stay isolated by project scope.
   */
  function resolveUsageContextKey(
    context?: SkillRunPayload["context"],
  ): string {
    const projectId = context?.projectId?.trim() ?? "";
    if (projectId.length > 0) {
      return `project:${projectId}`;
    }
    const documentId = context?.documentId?.trim() ?? "";
    if (documentId.length > 0) {
      return `document:${documentId}`;
    }
    return "global";
  }

  /**
   * Aggregate usage stats and optionally estimate cost when pricing exists.
   */
  function buildUsage(args: {
    model: string;
    context?: SkillRunPayload["context"];
    promptTokens: number;
    completionTokens: number;
  }): SkillRunUsage {
    const key = resolveUsageContextKey(args.context);
    const delta =
      Math.max(0, args.promptTokens) + Math.max(0, args.completionTokens);
    const nextTotal = (sessionTokenTotalsByContext.get(key) ?? 0) + delta;
    sessionTokenTotalsByContext.set(key, nextTotal);

    const pricing = modelPricingByModel.get(args.model.trim());
    const estimatedCostUsd =
      pricing === undefined
        ? undefined
        : Number(
            (
              (Math.max(0, args.promptTokens) / 1000) *
                pricing.promptPer1kTokens +
              (Math.max(0, args.completionTokens) / 1000) *
                pricing.completionPer1kTokens
            ).toFixed(6),
          );

    return {
      promptTokens: Math.max(0, args.promptTokens),
      completionTokens: Math.max(0, args.completionTokens),
      sessionTotalTokens: nextTotal,
      ...(typeof estimatedCostUsd === "number" ? { estimatedCostUsd } : {}),
    };
  }

  deps.ipcMain.handle(
    "ai:models:list",
    async (): Promise<IpcResponse<ModelCatalogResponse>> => {
      try {
        const res = await aiService.listModels();
        return res.ok
          ? { ok: true, data: res.data }
          : { ok: false, error: res.error };
      } catch (error) {
        deps.logger.error("ai_models_list_ipc_failed", {
          message: error instanceof Error ? error.message : String(error),
        });
        return {
          ok: false,
          error: { code: "INTERNAL", message: "AI models list failed" },
        };
      }
    },
  );
  deps.ipcMain.handle(
    "ai:skill:run",
    async (
      e,
      payload: SkillRunPayload,
    ): Promise<IpcResponse<SkillRunResponse>> => {
      if (payload.skillId.trim().length === 0) {
        return {
          ok: false,
          error: { code: "INVALID_ARGUMENT", message: "skillId is required" },
        };
      }
      if (payload.model.trim().length === 0) {
        return {
          ok: false,
          error: { code: "INVALID_ARGUMENT", message: "model is required" },
        };
      }
      if (!deps.db) {
        return {
          ok: false,
          error: createDbNotReadyError(),
        };
      }

      const candidateCountRes = parseCandidateCount(payload.candidateCount);
      if (!candidateCountRes.ok) {
        return {
          ok: false,
          error: candidateCountRes.error,
        };
      }
      const candidateCount = candidateCountRes.data;
      const effectiveStream = candidateCount > 1 ? false : payload.stream;
      const promptTokensForResult = estimateTokenCount(
        promptInputForUsage(payload),
      );

      const pushBackpressure = getPushBackpressureGate(e.sender);

      const emitEvent = (event: AiStreamEvent): void => {
        const eventToSend: AiStreamEvent =
          event.type === "done"
            ? {
                ...event,
                result: {
                  success: event.terminal === "completed",
                  output: event.outputText,
                  metadata: {
                    model: payload.model,
                    promptTokens: promptTokensForResult,
                    completionTokens: estimateTokenCount(event.outputText),
                  },
                  traceId: event.traceId,
                  ...(event.error ? { error: event.error } : {}),
                },
              }
            : event;

        if (!pushBackpressure.shouldDeliver(eventToSend)) {
          return;
        }

        safeEmitToRenderer({
          logger: deps.logger,
          sender: e.sender,
          event: eventToSend,
        });
      };

      const stats = createStatsService({ db: deps.db, logger: deps.logger });
      const inc = stats.increment({
        ts: nowTs(),
        delta: { skillsUsed: 1 },
      });
      if (!inc.ok) {
        deps.logger.error("stats_increment_skills_used_failed", {
          code: inc.error.code,
          message: inc.error.message,
        });
      }

      try {
        if (candidateCount === 1) {
          const res = await skillExecutor.execute({
            skillId: payload.skillId,
            input: payload.input,
            mode: payload.mode,
            model: payload.model,
            context: payload.context,
            stream: effectiveStream,
            ts: nowTs(),
            emitEvent,
          });
          if (!res.ok) {
            return { ok: false, error: res.error };
          }

          rememberRunId({ runId: res.data.runId, context: payload.context });

          const outputText = res.data.outputText;
          if (typeof outputText === "string") {
            const promptTokens = estimateTokenCount(
              promptInputForUsage(payload),
            );
            const completionTokens = estimateTokenCount(outputText);
            const usage = buildUsage({
              model: payload.model,
              context: payload.context,
              promptTokens,
              completionTokens,
            });
            const candidates: SkillRunCandidate[] = [
              {
                id: "candidate-1",
                runId: res.data.runId,
                text: outputText,
                summary: summarizeCandidateText(outputText),
              },
            ];

            return {
              ok: true,
              data: toSkillRunResponseData({
                ...res.data,
                candidates,
                usage,
                promptDiagnostics: payload.promptDiagnostics,
              }),
            };
          }

          return {
            ok: true,
            data: toSkillRunResponseData({
              ...res.data,
              promptDiagnostics: payload.promptDiagnostics,
            }),
          };
        }

        const runs: Array<{
          executionId: string;
          runId: string;
          outputText: string;
        }> = [];
        for (let index = 0; index < candidateCount; index += 1) {
          const res = await skillExecutor.execute({
            skillId: payload.skillId,
            input: payload.input,
            mode: payload.mode,
            model: payload.model,
            context: payload.context,
            stream: false,
            ts: nowTs(),
            emitEvent,
          });
          if (!res.ok) {
            return { ok: false, error: res.error };
          }
          rememberRunId({ runId: res.data.runId, context: payload.context });
          runs.push({
            executionId: res.data.executionId,
            runId: res.data.runId,
            outputText: res.data.outputText ?? "",
          });
        }

        const candidates: SkillRunCandidate[] = runs.map((item, index) => ({
          id: `candidate-${index + 1}`,
          runId: item.runId,
          text: item.outputText,
          summary: summarizeCandidateText(item.outputText),
        }));
        const completionTokens = runs.reduce(
          (sum, item) => sum + estimateTokenCount(item.outputText),
          0,
        );
        const promptTokens =
          estimateTokenCount(promptInputForUsage(payload)) * candidateCount;
        const usage = buildUsage({
          model: payload.model,
          context: payload.context,
          promptTokens,
          completionTokens,
        });
        const primary = runs[0];
        if (!primary) {
          return {
            ok: false,
            error: { code: "INTERNAL", message: "No candidates generated" },
          };
        }
        return {
          ok: true,
          data: toSkillRunResponseData({
            executionId: primary.executionId,
            runId: primary.runId,
            outputText: primary.outputText,
            candidates,
            usage,
            promptDiagnostics: payload.promptDiagnostics,
          }),
        };
      } catch (error) {
        deps.logger.error("ai_run_ipc_failed", {
          message: error instanceof Error ? error.message : String(error),
        });
        return {
          ok: false,
          error: { code: "INTERNAL", message: "AI run failed" },
        };
      }
    },
  );

  deps.ipcMain.handle(
    "ai:skill:cancel",
    async (
      _e,
      payload: { runId?: string; executionId?: string },
    ): Promise<IpcResponse<{ canceled: true }>> => {
      const executionId = (payload.executionId ?? payload.runId ?? "").trim();
      if (executionId.length === 0) {
        return {
          ok: false,
          error: {
            code: "INVALID_ARGUMENT",
            message: "executionId is required",
          },
        };
      }

      try {
        const res = aiService.cancel({
          executionId,
          runId: payload.runId,
          ts: nowTs(),
        });
        return res.ok
          ? { ok: true, data: res.data }
          : { ok: false, error: res.error };
      } catch (error) {
        deps.logger.error("ai_cancel_ipc_failed", {
          message: error instanceof Error ? error.message : String(error),
        });
        return {
          ok: false,
          error: { code: "INTERNAL", message: "AI cancel failed" },
        };
      }
    },
  );

  deps.ipcMain.handle(
    "ai:skill:feedback",
    async (
      _e,
      payload: SkillFeedbackPayload,
    ): Promise<IpcResponse<SkillFeedbackResponse>> => {
      if (payload.runId.trim().length === 0) {
        return {
          ok: false,
          error: { code: "INVALID_ARGUMENT", message: "runId is required" },
        };
      }

      if (!runRegistry.has(payload.runId)) {
        return {
          ok: false,
          error: { code: "NOT_FOUND", message: "runId not found" },
        };
      }

      if (!deps.db) {
        return {
          ok: false,
          error: createDbNotReadyError(),
        };
      }

      try {
        const memSvc = createMemoryService({
          db: deps.db,
          logger: deps.logger,
        });
        const settings = memSvc.getSettings();
        if (!settings.ok) {
          return { ok: false, error: settings.error };
        }

        const learning = recordSkillFeedbackAndLearn({
          db: deps.db,
          logger: deps.logger,
          settings: settings.data,
          runId: payload.runId,
          action: payload.action,
          evidenceRef: payload.evidenceRef,
        });
        if (!learning.ok) {
          return { ok: false, error: learning.error };
        }

        const res = aiService.feedback({
          runId: payload.runId,
          action: payload.action,
          evidenceRef: payload.evidenceRef,
          ts: nowTs(),
        });
        return res.ok
          ? {
              ok: true,
              data: {
                recorded: true,
                learning: {
                  ignored: learning.data.ignored,
                  ignoredReason: learning.data.ignoredReason,
                  learned: learning.data.learned,
                  learnedMemoryId: learning.data.learnedMemoryId,
                  signalCount: learning.data.signalCount,
                  threshold: learning.data.threshold,
                },
              },
            }
          : { ok: false, error: res.error };
      } catch (error) {
        deps.logger.error("ai_feedback_ipc_failed", {
          message: error instanceof Error ? error.message : String(error),
        });
        return {
          ok: false,
          error: { code: "INTERNAL", message: "AI feedback failed" },
        };
      }
    },
  );

  deps.ipcMain.handle(
    "ai:chat:send",
    async (
      _e,
      payload: ChatSendPayload,
    ): Promise<IpcResponse<ChatSendResponse>> => {
      const message = payload.message.trim();
      if (message.length === 0) {
        return {
          ok: false,
          error: { code: "INVALID_ARGUMENT", message: "message is required" },
        };
      }

      const projectId = resolveChatProjectId({ projectId: payload.projectId });
      if (!projectId.ok) {
        return {
          ok: false,
          error: projectId.error,
        };
      }

      const timestamp = nowTs();
      const projectMessages = chatHistoryByProject.get(projectId.data) ?? [];
      if (projectMessages.length >= AI_CHAT_MESSAGE_CAPACITY) {
        return {
          ok: false,
          error: {
            code: "CONFLICT",
            message: "会话消息已达上限，请先归档旧会话后继续",
          },
        };
      }
      const messageId = `chat-${timestamp}`;
      const nextMessage: ChatHistoryMessage = {
        messageId,
        projectId: projectId.data,
        role: "user",
        content: message,
        timestamp,
        traceId: `trace-${messageId}`,
      };
      const nextMessages = [...projectMessages, nextMessage];
      chatHistoryByProject.set(projectId.data, nextMessages);

      return {
        ok: true,
        data: {
          accepted: true,
          messageId,
          echoed: message,
        },
      };
    },
  );

  deps.ipcMain.handle(
    "ai:chat:list",
    async (
      _e,
      payload: ChatListPayload,
    ): Promise<IpcResponse<ChatListResponse>> => {
      const projectId = resolveChatProjectId({ projectId: payload.projectId });
      if (!projectId.ok) {
        return {
          ok: false,
          error: projectId.error,
        };
      }

      const messages = chatHistoryByProject.get(projectId.data) ?? [];
      return {
        ok: true,
        data: { items: [...messages] },
      };
    },
  );

  deps.ipcMain.handle(
    "ai:chat:clear",
    async (
      _e,
      payload: ChatClearPayload,
    ): Promise<IpcResponse<ChatClearResponse>> => {
      const projectId = resolveChatProjectId({ projectId: payload.projectId });
      if (!projectId.ok) {
        return {
          ok: false,
          error: projectId.error,
        };
      }

      const removed = chatHistoryByProject.get(projectId.data)?.length ?? 0;
      chatHistoryByProject.delete(projectId.data);

      return {
        ok: true,
        data: {
          cleared: true,
          removed,
        },
      };
    },
  );
}
